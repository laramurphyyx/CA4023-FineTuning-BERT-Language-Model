{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (4.8.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.9.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from newspaper3k) (4.6.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from newspaper3k) (4.8.0)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/17/3591f3a22fa98e86794fe6388245226dd0292cdc701fef3c4bda048c068e/tldextract-3.2.0-py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from newspaper3k) (2.22.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from newspaper3k) (3.4.5)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from newspaper3k) (6.2.0)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from newspaper3k) (5.1.2)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.0)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/d8/b2/15bf6781a861bbc5dd801d467f26448fb322bfedcd30f2e62b148d104dfb/feedparser-6.0.8-py3-none-any.whl (81kB)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (1.9.3)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.25.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2019.9.11)\n",
      "Requirement already satisfied: six in c:\\users\\laram\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.12.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
      "Building wheels for collected packages: feedfinder2, tinysegmenter, jieba3k, sgmllib3k\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3362 sha256=b5ef01a67b845629b172ad571a94cde27235ce1fc20cacf5e0a91935ac6bd1b7\n",
      "  Stored in directory: C:\\Users\\laram\\AppData\\Local\\pip\\Cache\\wheels\\de\\03\\ca\\778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13542 sha256=74ee566d9b62065adc6a8fcd037cab579f69b92eb1c50b550c2cc7b9f53b6355\n",
      "  Stored in directory: C:\\Users\\laram\\AppData\\Local\\pip\\Cache\\wheels\\81\\2b\\43\\a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398414 sha256=b03815ed87f18802d93da3be2fb6f98e951f1791ddfac7406a823bedcfa1fc57\n",
      "  Stored in directory: C:\\Users\\laram\\AppData\\Local\\pip\\Cache\\wheels\\83\\15\\9c\\a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6070 sha256=c862b45980670466e5d8c51a5427254648cc762a2fbcd84a7a2039e160650a9e\n",
      "  Stored in directory: C:\\Users\\laram\\AppData\\Local\\pip\\Cache\\wheels\\f1\\80\\5a\\444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
      "Successfully built feedfinder2 tinysegmenter jieba3k sgmllib3k\n",
      "Installing collected packages: requests-file, tldextract, feedfinder2, tinysegmenter, cssselect, jieba3k, sgmllib3k, feedparser, newspaper3k\n",
      "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_dataset = pd.read_excel('Fake News Stories.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article Number</th>\n",
       "      <th>URL of article</th>\n",
       "      <th>Fake or Satire?</th>\n",
       "      <th>URL of rebutting article</th>\n",
       "      <th>Fake or Satire?.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>http://www.redflagnews.com/headlines-2016/cdc-...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>http://www.snopes.com/cdc-forced-vaccinations/</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>376</td>\n",
       "      <td>http://www.redflagnews.com/headlines-2016/-out...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>http://www.snopes.com/white-house-logo-change/</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>377</td>\n",
       "      <td>http://www.redflagnews.com/headlines-2016/whit...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>http://www.snopes.com/obama-veterans-money-to-...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>378</td>\n",
       "      <td>http://www.redflagnews.com/headlines-2016/obam...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>http://www.snopes.com/obama-veterans-money-to-...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>379</td>\n",
       "      <td>http://www.redflagnews.com/headlines-2016/cali...</td>\n",
       "      <td>Fake</td>\n",
       "      <td>http://www.snopes.com/california-to-jail-clima...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>308</td>\n",
       "      <td>http://worldnewsdailyreport.com/aliens-meddled...</td>\n",
       "      <td>Satire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>309</td>\n",
       "      <td>http://worldnewsdailyreport.com/woman-runs-ove...</td>\n",
       "      <td>Satire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>314</td>\n",
       "      <td>https://www.suchpolitics.com/the-archives/2017...</td>\n",
       "      <td>Satire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>315</td>\n",
       "      <td>https://www.suchpolitics.com/the-archives/2017...</td>\n",
       "      <td>Satire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>325</td>\n",
       "      <td>https://www.mcsweeneys.net/articles/why-wont-y...</td>\n",
       "      <td>Satire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Satire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Article Number                                     URL of article  \\\n",
       "0               375  http://www.redflagnews.com/headlines-2016/cdc-...   \n",
       "1               376  http://www.redflagnews.com/headlines-2016/-out...   \n",
       "2               377  http://www.redflagnews.com/headlines-2016/whit...   \n",
       "3               378  http://www.redflagnews.com/headlines-2016/obam...   \n",
       "4               379  http://www.redflagnews.com/headlines-2016/cali...   \n",
       "..              ...                                                ...   \n",
       "487             308  http://worldnewsdailyreport.com/aliens-meddled...   \n",
       "488             309  http://worldnewsdailyreport.com/woman-runs-ove...   \n",
       "489             314  https://www.suchpolitics.com/the-archives/2017...   \n",
       "490             315  https://www.suchpolitics.com/the-archives/2017...   \n",
       "491             325  https://www.mcsweeneys.net/articles/why-wont-y...   \n",
       "\n",
       "    Fake or Satire?                           URL of rebutting article  \\\n",
       "0              Fake     http://www.snopes.com/cdc-forced-vaccinations/   \n",
       "1              Fake     http://www.snopes.com/white-house-logo-change/   \n",
       "2              Fake  http://www.snopes.com/obama-veterans-money-to-...   \n",
       "3              Fake  http://www.snopes.com/obama-veterans-money-to-...   \n",
       "4              Fake  http://www.snopes.com/california-to-jail-clima...   \n",
       "..              ...                                                ...   \n",
       "487         Satire                                                 NaN   \n",
       "488         Satire                                                 NaN   \n",
       "489         Satire                                                 NaN   \n",
       "490         Satire                                                 NaN   \n",
       "491         Satire                                                 NaN   \n",
       "\n",
       "    Fake or Satire?.1  \n",
       "0                Fake  \n",
       "1                Fake  \n",
       "2                Fake  \n",
       "3                Fake  \n",
       "4                Fake  \n",
       "..                ...  \n",
       "487           Satire   \n",
       "488           Satire   \n",
       "489           Satire   \n",
       "490           Satire   \n",
       "491           Satire   \n",
       "\n",
       "[492 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not access Article 8\n",
      "Could not access Article 9\n",
      "Could not access Article 10\n",
      "Could not access Article 23\n",
      "Could not access Article 24\n",
      "Could not access Article 25\n",
      "Could not access Article 26\n",
      "Could not access Article 27\n",
      "Could not access Article 38\n",
      "Could not access Article 40\n",
      "Could not access Article 41\n",
      "Could not access Article 42\n",
      "Could not access Article 53\n",
      "Could not access Article 54\n",
      "Could not access Article 55\n",
      "Could not access Article 56\n",
      "Could not access Article 57\n",
      "Could not access Article 58\n",
      "Could not access Article 59\n",
      "Could not access Article 60\n",
      "Could not access Article 61\n",
      "Could not access Article 68\n",
      "Could not access Article 72\n",
      "Could not access Article 78\n",
      "Could not access Article 79\n",
      "Could not access Article 90\n",
      "Could not access Article 91\n",
      "Could not access Article 92\n",
      "Could not access Article 93\n",
      "Could not access Article 94\n",
      "Could not access Article 113\n",
      "Could not access Article 142\n",
      "Could not access Article 151\n",
      "Could not access Article 152\n",
      "Could not access Article 153\n",
      "Could not access Article 154\n",
      "Could not access Article 155\n",
      "Could not access Article 157\n",
      "Could not access Article 158\n",
      "Could not access Article 159\n",
      "Could not access Article 170\n",
      "Could not access Article 171\n",
      "Could not access Article 172\n",
      "Could not access Article 186\n",
      "Could not access Article 190\n",
      "Could not access Article 216\n",
      "Could not access Article 217\n",
      "Could not access Article 218\n",
      "Could not access Article 219\n",
      "Could not access Article 220\n",
      "Could not access Article 227\n",
      "Could not access Article 229\n",
      "Could not access Article 230\n",
      "Could not access Article 231\n",
      "Could not access Article 269\n",
      "Could not access Article 270\n",
      "Could not access Article 286\n",
      "Could not access Article 310\n",
      "Could not access Article 313\n",
      "Could not access Article 318\n",
      "Could not access Article 319\n",
      "Could not access Article 320\n",
      "Could not access Article 321\n",
      "Could not access Article 322\n",
      "Could not access Article 327\n",
      "Could not access Article 337\n",
      "Could not access Article 338\n",
      "Could not access Article 339\n",
      "Could not access Article 348\n",
      "Could not access Article 354\n",
      "Could not access Article 357\n",
      "Could not access Article 358\n",
      "Could not access Article 370\n",
      "Could not access Article 384\n",
      "Could not access Article 391\n",
      "Could not access Article 392\n",
      "Could not access Article 393\n",
      "Could not access Article 394\n",
      "Could not access Article 395\n",
      "Could not access Article 396\n",
      "Could not access Article 397\n",
      "Could not access Article 398\n",
      "Could not access Article 399\n",
      "Could not access Article 419\n",
      "Could not access Article 420\n",
      "Could not access Article 421\n",
      "Could not access Article 422\n",
      "Could not access Article 431\n",
      "Could not access Article 446\n",
      "Could not access Article 449\n",
      "Could not access Article 450\n",
      "Could not access Article 456\n",
      "Could not access Article 457\n",
      "Could not access Article 468\n",
      "Could not access Article 469\n",
      "Could not access Article 477\n",
      "Could not access Article 478\n",
      "Could not access Article 483\n",
      "Could not access Article 485\n",
      "Could not access Article 486\n",
      "Could not access Article 504\n",
      "Could not access Article 506\n",
      "Could not access Article 538\n",
      "Could not access Article 539\n",
      "Could not access Article 540\n",
      "Could not access Article 541\n",
      "Could not access Article 542\n",
      "Could not access Article 544\n",
      "Could not access Article 545\n",
      "Could not access Article 546\n",
      "Could not access Article 547\n",
      "Could not access Article 548\n",
      "Could not access Article 560\n",
      "Could not access Article 561\n",
      "Could not access Article 562\n",
      "Could not access Article 563\n",
      "Could not access Article 564\n",
      "Could not access Article 565\n",
      "Could not access Article 586\n",
      "Could not access Article 587\n",
      "Could not access Article 156\n",
      "Could not access Article 344\n",
      "Could not access Article 150\n",
      "Could not access Article 228\n",
      "Could not access Article 28\n",
      "Could not access Article 29\n",
      "Could not access Article 125\n",
      "Could not access Article 126\n",
      "Could not access Article 127\n",
      "Could not access Article 128\n",
      "Could not access Article 129\n",
      "Could not access Article 134\n",
      "Could not access Article 135\n",
      "Could not access Article 136\n",
      "Could not access Article 137\n",
      "Could not access Article 138\n",
      "Could not access Article 145\n",
      "Could not access Article 146\n",
      "Could not access Article 148\n",
      "Could not access Article 165\n",
      "Could not access Article 166\n",
      "Could not access Article 167\n",
      "Could not access Article 168\n",
      "Could not access Article 169\n",
      "Could not access Article 206\n",
      "Could not access Article 207\n",
      "Could not access Article 209\n",
      "Could not access Article 265\n",
      "Could not access Article 266\n",
      "Could not access Article 267\n",
      "Could not access Article 289\n",
      "Could not access Article 335\n",
      "Could not access Article 336\n",
      "Could not access Article 346\n",
      "Could not access Article 347\n",
      "Could not access Article 369\n",
      "Could not access Article 429\n",
      "Could not access Article 458\n",
      "Could not access Article 459\n",
      "Could not access Article 460\n",
      "Could not access Article 461\n",
      "Could not access Article 462\n",
      "Could not access Article 513\n",
      "Could not access Article 514\n",
      "Could not access Article 516\n",
      "Could not access Article 517\n",
      "Could not access Article 566\n",
      "Could not access Article 567\n",
      "Could not access Article 568\n",
      "Could not access Article 569\n",
      "Could not access Article 570\n",
      "Could not access Article 575\n",
      "Could not access Article 592\n",
      "Could not access Article 593\n",
      "Could not access Article 594\n",
      "Could not access Article 595\n",
      "Could not access Article 30\n",
      "Could not access Article 305\n",
      "Could not access Article 307\n",
      "Could not access Article 308\n",
      "Could not access Article 309\n",
      "Could not access Article 314\n",
      "Could not access Article 315\n"
     ]
    }
   ],
   "source": [
    "dataset = {'article_id': [],\n",
    "           'article_content': [],\n",
    "           'fake_or_satire': []}\n",
    "\n",
    "for index, row in raw_dataset.iterrows():\n",
    "    \n",
    "    url = row['URL of article']\n",
    "    \n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    try: \n",
    "        article.parse()\n",
    "    except:\n",
    "        print(\"Could not access Article \" + str(row['Article Number']))\n",
    "        pass\n",
    "    \n",
    "    dataset['article_id'].append(row['Article Number'])\n",
    "    dataset['fake_or_satire'].append(row['Fake or Satire?'])\n",
    "    dataset['article_content'].append(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_links = input()\n",
    "\n",
    "broken_articles = broken_links.split(\"Could not access Article \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['article_content'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.dropna(subset=['article_content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping useless links\n",
    "dataframe = dataframe[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.drop(columns=['article_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('cleaned_fake_news_stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataframe.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = shuffled_dataset[:176]\n",
    "test_dataset = shuffled_dataset[176:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_csv('train_fake_news_stories.csv')\n",
    "test_dataset.to_csv('test_fake_news_stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
