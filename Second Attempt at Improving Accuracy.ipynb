{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\laram\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a335148facd46aa8054c43f460a327a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method used to classify the sentiment of a review in [this notebook](https://colab.research.google.com/drive/17LTkRgS5FaLsl2vKLiTB7R93MbQMLUuu?usp=sharing) is by creating vectors based on word counts, and training a logistic regression model on these vectors and their corresponding labels. The CountVectoriser Function from python's sklearn packages is the one used in this explanation. \n",
    "\n",
    "I will try to increase the accuracy by assessing the functionality of the CountVectoriser and assessing whether stop words or stemming can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Functionality of CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectoriser function removes punctuation from the review, changes it to lowercase, takes the 200 most common words that appear in the reviews, and transform each review into a 200-length vector containing the counts for each word in a review (as explained here by [GeeksForGeeks](https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/#:~:text=CountVectorizer%20creates%20a%20matrix%20in,in%20that%20particular%20text%20sample.) and from an article on [Medium](https://medium.com/@vasista/preparing-the-text-data-with-scikit-learn-b31a3df567e#:~:text=We%20can%20use%20CountVectorizer%20of,punctuation%20and%20lower%20the%20documents.&text=It%20turns%20each%20vector%20into,the%20word%20in%20the%20vocabulary.).\n",
    "\n",
    "To ensure I understand the way this function works, I will try to replicate this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets['train']\n",
    "train_data = []\n",
    "train_data_labels = []\n",
    "\n",
    "# Limiting the size of the dataset to 3, to allow for easier comparison\n",
    "i = 0\n",
    "for item in train_dataset:\n",
    "    train_data.append(item['text'])\n",
    "    train_data_labels.append(item['label'])\n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',max_features=20,lowercase=True)\n",
    "features = vectorizer.fit_transform(train_data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  2,  6,  1,  3,  4, 10,  4,  6,  1,  2,  6,  2,  0,  5,  4,\n",
       "        10,  4,  9,  1],\n",
       "       [ 5,  3,  0,  3,  2,  3,  7,  3,  1,  1,  3,  5,  3,  2,  0,  2,\n",
       "         9,  2,  3,  3],\n",
       "       [ 0,  0,  6,  2,  3,  1,  1,  1,  4,  2,  0,  1,  1,  6,  0,  0,\n",
       "         2,  3,  1,  2]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Counts of Words in all Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'are', 'br', 'but', 'film', 'for', 'in', 'is', 'it', 'no', 'nudity', 'of', 'on', 'one', 'sex', 'that', 'the', 'this', 'to', 'with']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 21), ('in', 18), ('to', 13), ('and', 13), ('of', 12), ('a', 12), ('it', 10), ('this', 9), ('i', 8), ('for', 8), ('is', 8), ('film', 7), ('that', 6), ('br', 6), ('on', 6), ('with', 6), ('but', 6), ('sex', 5), ('nudity', 5), ('are', 5)]\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for review in small_train_data:\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "    review = review.lower().split(\" \")\n",
    "    for word in review:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "            \n",
    "print(sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists seem to contain the mostly the same vocabulary.\n",
    "\n",
    "The CountVectoriser does not contain the words 'a' and 'i', it's possible that as part of the CountVectoriser function, it removes any words with a length less than 0. \n",
    "\n",
    "Let's try implement this into our manual analysis and our most common words may not contain 'a' and 'i', but rather 'no' and 'one', as seen in the CountVectorizer vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 21), ('in', 18), ('to', 13), ('and', 13), ('of', 12), ('it', 10), ('this', 9), ('for', 8), ('is', 8), ('film', 7), ('that', 6), ('br', 6), ('on', 6), ('with', 6), ('but', 6), ('sex', 5), ('nudity', 5), ('are', 5), ('one', 5), ('am', 4)]\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for review in small_train_data:\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "    review = review.lower().split(\" \")\n",
    "    for word in review:\n",
    "        if len(word) > 1:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "            \n",
    "print(sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both lists contain 19/20 of the words in common. The manual version contains the word 'am', and the CountVectorizer contains the word 'no'. This is due to both words having a frequency of 4. It is safe to assume that this is roughly how the CountVectoriser function works in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Impact of Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',max_features=20,lowercase=True, stop_words='english')\n",
    "features = vectorizer.fit_transform(train_data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', 'comes', 'considered', 'curious', 'doesn', 'double', 'drama', 'film', 'films', 'issues', 'like', 'making', 'men', 'nudity', 'really', 'sex', 'shown', 'standard', 'swedish', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that removing stop words has improved the amount of meaningful words in the WordVectoriser feature count. \n",
    "\n",
    "Let's try this at a large scale, on all 50,000 rows of data, to test if there is an improvement to accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an 80/20 split for training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning all Data to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "all_data_labels = []\n",
    "for item in raw_datasets['train']:\n",
    "    all_data.append(item['text'])\n",
    "    all_data_labels.append(item['label'])\n",
    "\n",
    "for item in raw_datasets['train']:\n",
    "    all_data.append(item['text'])\n",
    "    all_data_labels.append(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ids = random.sample(list(np.arange(0,50000)), 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_data_labels = []\n",
    "test_data = []\n",
    "test_data_labels = []\n",
    "\n",
    "for review_id in range(0,50000):\n",
    "    if review_id in training_ids:\n",
    "        train_data.append(all_data[review_id])\n",
    "        train_data_labels.append(all_data_labels[review_id])\n",
    "    else:\n",
    "        test_data.append(all_data[review_id])\n",
    "        test_data_labels.append(all_data_labels[review_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure that the reviews are evenly distributed between the training set and the test set to ensure there is no bias that can affect the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of positive reviews in the training set is 50.002500000000005%\n",
      "The percentage of positive reviews in the testing set is 49.99%\n"
     ]
    }
   ],
   "source": [
    "positive_training = sum(train_data_labels) / len(train_data_labels) * 100\n",
    "positive_testing = sum(test_data_labels) / len(test_data_labels) * 100\n",
    "    \n",
    "print(\"The percentage of positive reviews in the training set is \" + str(positive_training) + \"%\")\n",
    "print(\"The percentage of positive reviews in the testing set is \" + str(positive_testing) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentages are close enough to 50% to allow the distribution to be considered fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', max_features=200, lowercase=True, stop_words='english')\n",
    "features = vectorizer.fit_transform(train_data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=features_nd,y=train_data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning the Test Data and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:291: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices = (scores > 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "test_pred=log_model.predict(vectorizer.transform(test_data).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7813\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_pred,test_data_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 78.13% is a slight improvement from the accuracy obtained using the 80/20 split in [this notebook](https://github.com/laramurphyyx/CA4023_Assignment2/blob/main/First%20Attempt%20at%20Improving%20Accuracy.ipynb), which was 76.77%.\n",
    "\n",
    "Stop words are unlikely to influence the classification of a review in a meaningful way. As the most common 200 words in reviews will likely contain many stop words, it is reducing the number of influential words being chosen as a vector feature. This would imply that there would be a significant improvement in accuracy. It is difficult to explain why this did not occur. It could be that the stop words had no affect on the classification of positive/negative reviews, and so removing them and replacing them with less common 'meaningful' words.\n",
    "\n",
    "It's also possible that the most common words between all reviews are unevenly distributed between positive and negative words. The next step is to create a 200-length vector of the 100 most common words in positive reviews and the 100 most common words in negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Distribution of the Word Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_data = []\n",
    "negative_train_data = []\n",
    "\n",
    "for i in range(0,40000):\n",
    "    if train_data_labels[i]:\n",
    "        positive_train_data.append(train_data[i])\n",
    "    else:\n",
    "        negative_train_data.append(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive Vector\n",
    "vectorizer_positive = CountVectorizer(analyzer='word', max_features=150, lowercase=True, stop_words='english')\n",
    "features_positive = vectorizer_positive.fit_transform(positive_train_data)\n",
    "positive_vocabulary = vectorizer_positive.vocabulary_\n",
    "\n",
    "# Negative Vector\n",
    "vectorizer_negative = CountVectorizer(analyzer='word', max_features=150, lowercase=True, stop_words='english')\n",
    "features_negative = vectorizer_negative.fit_transform(negative_train_data)\n",
    "negative_vocabulary = vectorizer_negative.vocabulary_\n",
    "\n",
    "# Combining both vectors and re-indexing the vector\n",
    "vocabulary = {**positive_vocabulary, **negative_vocabulary}\n",
    "temp = []\n",
    "unique_vocabulary = dict()\n",
    "i = 0\n",
    "for key, val in vocabulary.items():\n",
    "    vocabulary[key] = i\n",
    "    i += 1\n",
    "\n",
    "# Using CountVectorizer with this new vocabulary\n",
    "vectorizer_combined = CountVectorizer(vocabulary=vocabulary, lowercase=True, stop_words='english')\n",
    "features = vectorizer_combined.fit_transform(train_data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=features_nd,y=train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=features_nd,y=train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:291: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices = (scores > 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "test_pred=log_model.predict(vectorizer_combined.transform(test_data).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8008\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_pred,test_data_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80.08% is the highest accuracy that we have achieved as of yet. This is expected as there is an evenly distributed number of words in the vector that are positive and negative. \n",
    "\n",
    "Let's see if applying stemming will increase the accuracy further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
