{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Relevant Packages and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\laram\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8639e89cfb4b75a2b7128fe9ac4dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method used to classify the sentiment of a review in [this notebook](https://colab.research.google.com/drive/17LTkRgS5FaLsl2vKLiTB7R93MbQMLUuu?usp=sharing) is by creating vectors based on word counts, and training a logistic regression model on these vectors and their corresponding labels. The CountVectoriser Function from python's sklearn packages is the one used in this explanation. \n",
    "\n",
    "I will try to increase the accuracy by assessing the functionality of the CountVectoriser and assessing whether stop words or stemming can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CountVectorizer to Improve Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Preparation:</u> Investigating CountVectorizer Functionality</b>\n",
    "    \n",
    "To begin this analysis, it's important to understand how the CountVectorizer works to identify any areas for possible optimization. \n",
    "\n",
    "The CountVectorizer function removes punctuation from the review, changes it to lowercase, takes the 200 most common words that appear in the reviews, and transform each review into a 200-length vector containing the counts for each word in a review (as explained here by [GeeksForGeeks](https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/#:~:text=CountVectorizer%20creates%20a%20matrix%20in,in%20that%20particular%20text%20sample.) and from an article on [Medium](https://medium.com/@vasista/preparing-the-text-data-with-scikit-learn-b31a3df567e#:~:text=We%20can%20use%20CountVectorizer%20of,punctuation%20and%20lower%20the%20documents.&text=It%20turns%20each%20vector%20into,the%20word%20in%20the%20vocabulary.)).\n",
    "\n",
    "To ensure I understand the way this function works, I will try to replicate this manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Part 1:</u> Removing Stop Words</b>\n",
    "\n",
    "After replicating the results achieved by CountVectorizer on a small scale, we can measure the impact of removing stop words. This will make use of CountVectorizer's built in 'stop_words' function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Part 2:</u> Re-Distributing the CountVectorizer input</b>\n",
    "\n",
    "The CountVectorizer currently takes the 200 most common words in <u>all</u> reviews. This may not be the most effective way of creating vectors. It's not obvious the distribution of these common words, it's possible that there is an uneven distribution of positively-associated and negatively-associated words being included in the vector.\n",
    "\n",
    "To accomodate for this condition, the CountVecoriser function is given a custom vocabulary to use. This custom vocabulary consists of the 150 most common words in the positive reviews and the 150 most common words in the negative reviews. The overall vocabulary is the intersection of these sets of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><u>Part 3:</u> Applying Stemming to the Words</b>\n",
    "\n",
    "Applying stemming may increase accuracy as the most common words chosen by the countVectoriser may contain many words with the same stem. For example, 'love' and 'loved' may be have a high frequency, but they have similar meanings and should be represented as the same word in the vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation: Exploring the Functionality of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets['train']\n",
    "train_data = []\n",
    "train_data_labels = []\n",
    "\n",
    "# Limiting the size of the dataset to 3, to allow for easier comparison\n",
    "i = 0\n",
    "for item in train_dataset:\n",
    "    train_data.append(item['text'])\n",
    "    train_data_labels.append(item['label'])\n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',max_features=20,lowercase=True)\n",
    "features = vectorizer.fit_transform(train_data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  2,  6,  1,  3,  4, 10,  4,  6,  1,  2,  6,  2,  0,  5,  4,\n",
       "        10,  4,  9,  1],\n",
       "       [ 5,  3,  0,  3,  2,  3,  7,  3,  1,  1,  3,  5,  3,  2,  0,  2,\n",
       "         9,  2,  3,  3],\n",
       "       [ 0,  0,  6,  2,  3,  1,  1,  1,  4,  2,  0,  1,  1,  6,  0,  0,\n",
       "         2,  3,  1,  2]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Counts of Words in all Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'are', 'br', 'but', 'film', 'for', 'in', 'is', 'it', 'no', 'nudity', 'of', 'on', 'one', 'sex', 'that', 'the', 'this', 'to', 'with']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 21), ('in', 18), ('to', 13), ('and', 13), ('of', 12), ('a', 12), ('it', 10), ('this', 9), ('i', 8), ('for', 8), ('is', 8), ('film', 7), ('that', 6), ('br', 6), ('on', 6), ('with', 6), ('but', 6), ('sex', 5), ('nudity', 5), ('are', 5)]\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for review in small_train_data:\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "    review = review.lower().split(\" \")\n",
    "    for word in review:\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "            \n",
    "print(sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists seem to contain the mostly the same vocabulary.\n",
    "\n",
    "The CountVectoriser does not contain the words 'a' and 'i', it's possible that as part of the CountVectoriser function, it removes any words with a length less than 2. \n",
    "\n",
    "Let's try implement this into our manual analysis and our most common words may not contain 'a' and 'i', but rather 'no' and 'one', as seen in the CountVectorizer vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 21), ('in', 18), ('to', 13), ('and', 13), ('of', 12), ('it', 10), ('this', 9), ('for', 8), ('is', 8), ('film', 7), ('that', 6), ('br', 6), ('on', 6), ('with', 6), ('but', 6), ('sex', 5), ('nudity', 5), ('are', 5), ('one', 5), ('am', 4)]\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for review in small_train_data:\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "    review = review.lower().split(\" \")\n",
    "    for word in review:\n",
    "        if len(word) > 1:\n",
    "            if word in word_dict:\n",
    "                word_dict[word] += 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "            \n",
    "print(sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both lists contain 19/20 of the words in common. The manual version contains the word 'am', and the CountVectorizer contains the word 'no'. This is due to both words having a frequency of 4. It is safe to assume that this is roughly how the CountVectoriser function works in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Removing Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 : Exploring the Impact of Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',max_features=20,lowercase=True, stop_words='english')\n",
    "features = vectorizer.fit_transform(train_data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', 'comes', 'considered', 'curious', 'doesn', 'double', 'drama', 'film', 'films', 'issues', 'like', 'making', 'men', 'nudity', 'really', 'sex', 'shown', 'standard', 'swedish', 'yellow']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that removing stop words has improved the amount of meaningful words in the WordVectoriser feature count. \n",
    "\n",
    "Let's try this at a large scale, on all 50,000 rows of data, to test if there is an improvement to accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 : Removing Stop Words from Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\laram\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1\\cache-b23cfeb68a931a8d.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = raw_datasets['train']\n",
    "train_data = []\n",
    "train_data_labels = []\n",
    "for item in train_dataset:\n",
    "    train_data.append(item['text'])\n",
    "    train_data_labels.append(item['label'])\n",
    "\n",
    "test_dataset = raw_datasets['test'].shuffle(seed=42).select(range(1000)) # randomly selecting 1000 / 25000\n",
    "test_data = []\n",
    "test_data_labels = []\n",
    "for item in test_dataset:\n",
    "    test_data.append(item['text'])\n",
    "    test_data_labels.append(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', max_features=200, lowercase=True, stop_words='english')\n",
    "features = vectorizer.fit_transform(train_data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=features_nd,y=train_data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning the Test Data and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:291: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices = (scores > 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "test_pred=log_model.predict(vectorizer.transform(test_data).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.779\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_pred,test_data_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 77.9% is a decent improvement from the accuracy obtained [this notebook](https://colab.research.google.com/drive/17LTkRgS5FaLsl2vKLiTB7R93MbQMLUuu?usp=sharing), which was 74% - 76.66%.\n",
    "\n",
    "Stop words are unlikely to influence the classification of a review in a meaningful way. As the most common 200 words in reviews will likely contain many stop words, it is reducing the number of influential words being chosen as a vector feature. This would imply that there would be a significant improvement in accuracy. It is difficult to explain why this did not occur. It could be that the stop words had no affect on the classification of positive/negative reviews, and so removing them and replacing them with less common 'meaningful' words.\n",
    "\n",
    "It's also possible that the most common words between all reviews are unevenly distributed between positive and negative words. The next step is to create a new vector of the 150 most common words in positive reviews and the 150 most common words in negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Re-Distribution of the Word Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_data = []\n",
    "negative_train_data = []\n",
    "\n",
    "for i in range(0,25000):\n",
    "    if train_data_labels[i]:\n",
    "        positive_train_data.append(train_data[i])\n",
    "    else:\n",
    "        negative_train_data.append(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive Vector\n",
    "vectorizer_positive = CountVectorizer(analyzer='word', max_features=150, lowercase=True, stop_words='english')\n",
    "features_positive = vectorizer_positive.fit_transform(positive_train_data)\n",
    "positive_vocabulary = vectorizer_positive.vocabulary_\n",
    "\n",
    "# Negative Vector\n",
    "vectorizer_negative = CountVectorizer(analyzer='word', max_features=150, lowercase=True, stop_words='english')\n",
    "features_negative = vectorizer_negative.fit_transform(negative_train_data)\n",
    "negative_vocabulary = vectorizer_negative.vocabulary_\n",
    "\n",
    "# Combining both vectors and re-indexing the vector\n",
    "vocabulary = {**positive_vocabulary, **negative_vocabulary}\n",
    "temp = []\n",
    "unique_vocabulary = dict()\n",
    "i = 0\n",
    "for key, val in vocabulary.items():\n",
    "    vocabulary[key] = i\n",
    "    i += 1\n",
    "\n",
    "# Using CountVectorizer with this new vocabulary\n",
    "vectorizer_combined = CountVectorizer(vocabulary=vocabulary, lowercase=True, stop_words='english')\n",
    "features = vectorizer_combined.fit_transform(train_data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=features_nd,y=train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:291: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices = (scores > 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "test_pred=log_model.predict(vectorizer_combined.transform(test_data).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_pred,test_data_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80.2% is the highest accuracy that we have achieved as of yet. However, removing the stop words alone achieved an accuracy of 77.9%, so this is only an increase of 1.3%. An increase in accuracy was expected as there is an evenly distributed number of words in the vector that are positive and negative (or both). \n",
    "\n",
    "Let's see if applying stemming will increase the accuracy further. This is a less reliable method to improve accuracy as there is a risk of overtstemming or understemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Applying Porter-Stemmer Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_train_data = []\n",
    "stemmed_test_data = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for review in train_data:\n",
    "    review_stemmed_words = []\n",
    "    for word in review.split():\n",
    "        review_stemmed_words.append(ps.stem(word))\n",
    "    stemmed_train_data.append(\" \".join(review_stemmed_words))\n",
    "    \n",
    "for review in test_data:\n",
    "    review_stemmed_words = []\n",
    "    for word in review.split():\n",
    "        review_stemmed_words.append(ps.stem(word))\n",
    "    stemmed_test_data.append(\" \".join(review_stemmed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train_data = []\n",
    "negative_train_data = []\n",
    "\n",
    "for i in range(0,25000):\n",
    "    if train_data_labels[i]:\n",
    "        positive_train_data.append(train_data[i])\n",
    "    else:\n",
    "        negative_train_data.append(train_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive Vector\n",
    "vectorizer_positive = CountVectorizer(analyzer='word', max_features=150, lowercase=True, stop_words='english')\n",
    "features_positive = vectorizer_positive.fit_transform(positive_train_data)\n",
    "positive_vocabulary = vectorizer_positive.vocabulary_\n",
    "\n",
    "# Negative Vector\n",
    "vectorizer_negative = CountVectorizer(analyzer='word', max_features=150, lowercase=True, stop_words='english')\n",
    "features_negative = vectorizer_negative.fit_transform(negative_train_data)\n",
    "negative_vocabulary = vectorizer_negative.vocabulary_\n",
    "\n",
    "# Combining both vectors and re-indexing the vector\n",
    "vocabulary = {**positive_vocabulary, **negative_vocabulary}\n",
    "temp = []\n",
    "unique_vocabulary = dict()\n",
    "i = 0\n",
    "for key, val in vocabulary.items():\n",
    "    vocabulary[key] = i\n",
    "    i += 1\n",
    "\n",
    "# Using CountVectorizer with this new vocabulary\n",
    "vectorizer_combined = CountVectorizer(vocabulary=vocabulary, lowercase=True, stop_words='english')\n",
    "features = vectorizer_combined.fit_transform(train_data)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model = log_model.fit(X=features_nd,y=train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laram\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:291: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  indices = (scores > 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "test_pred=log_model.predict(vectorizer_combined.transform(test_data).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_pred,test_data_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 80.2% is the same accuracy achieved by removing stop words and restructuring the CountVectorizer vocabulary. \n",
    "\n",
    "This shows that the application of stemming the the reviews has had zero impact on the accuracy achieved by the model.\n",
    "\n",
    "This could be for various reasons, such as over-stemming or under-stemming, or possibly just the wide range of vocabulary being stemmed has not been fully exploited due to only choosing the 150 most common words of each sentiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
